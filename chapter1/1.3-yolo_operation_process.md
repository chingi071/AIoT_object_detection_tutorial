# YOLO 運作流程

YOLO 的整體運作流程分成三個步驟：

(1)	將圖片resize成模型需要的尺寸大小後輸入至 CNN 模型

(2)	透過 CNN 模型提取其特徵並輸出預測的 bounding box

(3)	將這些 bounding box 通過 NMS (Non-Maximum Suppression) 進行篩選，並結合預測的 class 得到最終結果

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/013.jpg)


那模型是怎麼偵測出物件的類別及位置資訊呢?

在圖片輸入至模型時，會將圖片劃分為 SxS 個 grid cell (假設S=7)，每個 cell 需要辨識出中心點位於該 cell 的物體，換句話說，若被偵測物體的中心落入某個 cell 內，則此 cell 就要負責去偵測該物體。

如下圖所示，偵測到貓咪的 bndBox 中心點位於粉紅框的 cell 內，則該 cell 會負責偵測這隻貓咪。藉由這些 cell 來偵測出物件的類別及位置資訊。

<div align=center><img width="400" height="500" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/014.jpg"/></div>


## 物件位置偵測

先來看位置資訊是怎麼實現的。

每個 cell 會負責預測出 B 個 bounding box (假設 B=2)，每個 bounding box 有 5 個預測值：x, y, w, h 以及 confidence (信心程度)，其中 x, y, w, h 為 bounding box 中心點的 x座標、y 座標、寬 (w)、高 (h)，並且皆是歸一化後的結果，因此值會介於 0～1 之間。

<div align=center><img width="500" height="400" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/015.jpg"/></div>


而 confidence 表示該 bounding box是否有物件以及預測位置 (x, y, w, h) 的準確程度。

公式定義如下圖，Pr(object) 是指 bounding box 是否包含物體，若有包含物體則為 1，反之則為 0。

IoU 是指 bounding box 與 Ground Truth 的 IoU 值，藉由該值能夠來判斷這個 bounding box 框出的是物體還是背景。

<div align=center><img width="700" height="100" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/016.jpg"/></div>


## 物件類別偵測

接著來看物件類別預測的實現，分類的預測一樣也是由每一個 cell 進行預測來得到。 

YOLOv1 和 YOLOv2 的輸出結果有點差異，YOLOv1 是在每個 cell 中預測出 2個 bounding box 及每個類別的概率值，假測有 20 個類別，就會輸出 20 個分類概率值；而 YOLOv2 則是在每個 cell 中預測出 5個 bounding box，每個 bounding box 會預測 x、y、w、h、confidence 及分類概率值。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/017.jpg)


現在我們知道了模型是如何偵測出 bounding box，由於每個 cell 都會預測出好多個 bounding box，有些可能被偵測為背景 (沒有框出任何物體)、有些可能有偵測到物件。但圖片中可能沒有那麼多的物件，需要消除一些重疊或是信心程度較小的 bounding box，因此會進行第三個步驟中的 NMS (Non-Maximum Suppression) 來篩選出最終結果。

## Non-Maximum Suppression（NMS）

NMS 是指用來挑選最佳 bounding box 的演算法，接著來看一下 NMS 的具體實現，這部分會分成三個步驟：

(1)	需要設定一個閾值 (threshold)，先對所有 bounding box 的 confidence score 進行過濾，若 score 小於 threshold 表示該 bounding box 可能為背景，則將該 bounding box 去除，可以減少 NMS 的計算量。

(2)	將 bounding box 根據 confidence 由高至低進行排序，confidence 最高的為最終輸出預測框，接著與其他 bounding box 計算 IoU，若大於 threshold 代表兩者框住同一個物體，則將 confidence較小的 bounding box 的 score 設為 0，即刪除此預測框；反之表示兩者框到不同物體，則留下該 bounding box。

(3)	重複此動作，直到剩下一個該類別的預測框，然後再計算下一個類別，直到全部類別都計算完畢。

接著來看一個例子會更清楚，將 threshold 設定為 0.5，第一步先將輸出的 bounding box 進行過濾，留下 confidence score 大於 threshold 的框。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/018.jpg)


第二步是將過濾完的 bounding box 進行排序，confidence score 最高的就是最終輸出預測框，下圖例子的最終輸出框是粉紅框。
將其他框跟粉紅框計算 IoU，並且將 IoU 大於 threshold 的 score 設定為 0，即表示刪除該框。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/019.jpg)


由於還有剩下一些框，表示這些框是其他物件的預測，因此再進行一次剛剛的動作 (第三步驟)。
如圖所示，黃色框為最終預測框，而目前沒有其他預測框了，表示已全部計算完畢。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/020.jpg)
