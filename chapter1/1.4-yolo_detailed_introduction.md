# YOLO 細部介紹

## 模型架構組成與演進

物件偵測模型通常分為以下部分：

* Input: 指圖片的輸入

* Backbone: 在ImageNet預訓練的骨架

* Neck: 通常用來提取不同層級的特徵圖，為了能夠擴大感受野和融合不同尺度feature map 的信息，以更好的進行特徵融合

* Head: 預測對象類別及bndBox的檢測器，通常分兩類 Dense Prediction (one stage), Sparse Prediction (two stage)

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/021.jpg)


YOLOv1、YOLOv2 中的模型架構參考於 GoogleNet，YOLOv3 在基於YOLOv2 的架構上提出了新的 backbone Darknet-53，如同其名，總共有53層convolutional layer；
使用 Feature Pyramid Network (FPN) 作為 Neck；Head 則沿用 YOLO Head，YOLO Head 是指用於偵測物體的網路架構。

YOLOv4 的模型架構分別使用 CSPDarknet53 作為 Backbone、SPP+PAN 作為 Neck、Head 一樣是沿用 YOLO Head。

針對不同的設備 (一般 GPU, 低運算 GPU, 高運算 GPU) 會選擇其合適的模型縮放大小，以達到推理運算速度與準確度上的 trade-off，因而開發了 YOLOv4-large、YOLOv4-CSP 和 YOLOv4-tiny 等網路架構。

經常使用的縮放因子有 resolution (圖像大小)、depth (模型層數)、width (通道數量)、stage (feature pyramid 數量)。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/022.jpg)


## Bag-of-Freebies、Bag-of-Specials

在訓練、測試模型時，經常會使用一些優化模型的技巧，稱為 tricks，在論文裡其分為兩大類: Bag-of-Freebies、Bag-of-Specials。

Bag-of-Freebies 是指在網絡訓練時所用到的技巧，不影響推理預測的時間，主要包括以下幾種

* 數據增強 (data augmentation): Random erase、CutOut、Hide-and-seek、Grid mask、GAN、MixUp、CutMix

* 正則化方法: DropOut、DropConnect

* 處理數據不平衡問題: focal loss、Online hard example mining、Hard negative example mining

* 處理 bndBox 回歸問題: MSE, IOU, GIOU, DIOU/CIOU

Bag-of-specials 是指在網絡設計或後處理時所用到的技巧，輕微增加推理預測時間，但可提升較大的精度，主要包括以下幾種

* 感受野: SPP、ASPP、RFB

* 特徵融合: FPN、PAN

* 注意力機制: attention module

* 激活函數: Swish、Mish

* NMS: Soft-NMS、DIoU NMS


## Feature Pyramid Network (FPN)

FPN 是一種自下而上、自上而下並橫向連接的網路結構，通過橫向連接可以融合不同層次的特徵，從而增強網路的特徵提取能力。連接的做法如下圖所示，特徵會先從下自上進行 2 倍的下採樣，而另一邊特徵會由上自下進行 2 倍的上采樣，接著使用 1x1 卷積層降低 channel 維度，使得兩邊維度一致後再做相加。

<div align=center><img width="500" height="400" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/023.jpg"/></div>


YOLO 借鑒了這個方法，並在不同尺度的特徵圖上進行，目的是為了針對不同大小的物體進行檢測，以提升小物體的預測能力。其做法是會通過下採樣32倍、16倍、8倍得到3個不同尺度的特徵圖，例如輸入416x416的圖片，則會得到13x13 (416/32)、26x26 (416/16)、52x52 (416/8)，這3個尺度的特徵圖。

由於 13x13 的特徵圖有最大的感受野，因此用於偵測大物體；26x26 的特徵圖是屬於中等的感受野，用於偵測中等大小的物體；而 52x52 的特徵圖則是擁有較小的感受野，用於偵測小物體。

每一個尺度的特徵圖會分別預測 3個 Anchor box，而 Anchor box 的大小會使用 K-means 進行聚類分析得到。在COCO資料集上，按照輸入圖片的尺寸為416x416，得到9種聚類結果：(10x13), (16x30), (33x23), (30x61), (62x45), (59x119), (116x90), (156x198), (373x326)，尺寸數字越大的 Anchor box 表示用於偵測越大的物體。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/024.jpg)


## Detect Flow

接著來看整體的網路偵測流程圖，以下圖 YOLOv3 為例，若輸入416x416的圖片，在79層卷積層後，會先經過32倍的下採樣，再通過3x3, 1x1的卷積層後，得到13x13的特徵圖 (第82層)。

下一步，會將第79層13x13的特徵圖進行上採樣，與第61層26x26的特徵圖合併 (Concat) 後，也就是使用 FPN 的方法。再經過16倍的下採樣及3x3, 1x1的卷積層後，得到26x26的特徵圖 (第94層)。

在第91層26x26的特徵圖會再次進行上採樣，並與第36層26x26的特徵圖合併 (Concat) 後，再經過8倍下採樣及3x3, 1x1的卷積層後，得到52x52的 特徵圖 (第106層)。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/025.jpg)


## Focal Loss

在 1.2 節有提到採用 Focal Loss 能解決正負樣本不均衡的問題，那他是怎麼進行的呢?

Focal Loss 的演變共分為三個步驟，首先基於二分類的 cross entropy (CE) 做改進，引入了權重系數 (Balanced Cross Entropy)。

* Balanced Cross Entropy

下圖 1 式為 cross entropy 的定義，其中 y 表示為 ground truth，1 為正樣本、反之為負樣本；p ∈ [0,1] 則是指預測為正樣本的機率值 (2 式)；接著將其簡寫如下。
由於 Focal Loss 想達成的目標是解決類別不平衡的問題，較直觀的想法是引入權重系數 α，此時就變成了 α-balanced CE loss。其中 α∈ [0,1]，對於正樣本給予 α 係數的權重、負樣本則給予 1-α 係數的權重。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/026.jpg)

* Focal Loss Definition-1

但 α-balanced CE loss 僅根據正負樣本進行平衡，並沒有考慮樣本的難易度，雖然降低了容易分類的負樣本損失，同時也讓模型更難分類較難的負樣本。

因此在第二步驟時，改為加上調節因子 -(1-pt)^γ，其中 γ≥ 0。藉由公式可以看到當樣本被誤分且 pt 很小時，調節因子接近 1，對 loss 不會產生影響；而對於被分類正確且 pt 接近 1 時，調節因子接近 0，可以有效降低對正確分類的 loss，讓模型更關注較難分類的樣本。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/027.jpg)

* Focal Loss Definition-2

最後，將第一步和第二步融合，除了調節因子外再加上權重係數，經實驗結果證明，效果比原始的更好，其中 γ=2,α=0.25 的組合效果最佳。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/028.jpg)


## Cross Stage Partial Network (CSPNet)

由於物件偵測經常會搭載運行於小型的機器上，因此低計算量、執行效率高是非常重要的一個目標。而CSPNet 架構是能夠在降低計算量的同時，使網路架構獲取更豐富的梯度融合信息，達到更好的預測效果。

做法是先將Base layer 劃分為2個部分，其中一部分會經過 Dense block 和 Transition layer (包含1x1卷積層及 pooling 層)，再融合起來。這樣的方法能夠帶來三項優點：

* 增加卷積層的學習能力，即便將模型輕量化，也能保持準確性

* 去掉計算力較高的計算瓶頸結構，即指降低計算量

* 降低內存占用

<div align=center><img width="400" height="300" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/029.jpg"/></div>


## Spatial Pyramid Pooling (SPP)、Path Aggregation Network (PAN)

SPP與 PAN 這兩者模型架構都是屬於 Bag-of-specials 方法。

SPP 的做法是在模型的最後一層把所有的特徵圖全部 concatenate 在一起，這樣的架構能讓模型再繼續接其他的網路層。

<div align=center><img width="500" height="600" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/030.jpg"/></div>


而 PAN 是基於 FPN 做改進，將串的層數再多加一層，並且把原本相加的部分修改為合併。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/031.jpg)

## Spatial Attention Module (SAM)

SAM 是基於 CBAM(Convolutional Block Attention Module) 做改進，在講解 SAM 之前，先來介紹 CBAM 架構。

* CBAM(Convolutional Block Attention Module) 

CBAM 是一種注意力機制模塊，採用兩種不同的注意力機制用於通道和空間之中，分別為 Channel attention module 以及 Spatial attention module。
前者用於尋找輸入圖像中 **什麼樣** 的特徵是需要關注的、後者則是用於尋找輸入圖像中 **哪裡** 的特徵是需要關注的。其架構簡單輕巧，能夠很輕易地整合至現有的網路架構中。

整體架構如下圖，首先輸入一個特徵F，先進行 Channel attention module 後得到權重系數與原來的特徵F相乘，然後再進行 Spatial attention module 後得到權重系數與原來的特徵F相乘，最後就可以得到縮放後的新特徵。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/032.jpg)


* Channel attention module 

CAM 如同其名，是將注意力機制用於通道中，期望能夠藉由注意力機制找出需要關注的特徵。

在聚合空間的訊息時，經常會使用 Average pooling 操作，例如: SENet。在 CBAM 論文中進行了許多實驗，提出同時與 Max pooling 一起使用能夠得到更好的結果，因此在 CAM 中會同時使用 Average pooling 及 Max pooling。

下圖為 CAM 的網路架構，首先會將輸入的特徵 F 分別進行全局的 Maxpooling 與 Averagepooling 後，各自通過一個權重共享的 MLP，再將這兩個輸出 vector 進行 element-wise summation 操作並通過 Sigmoid 得到權重系數 Mc。
最後再將這個權重系數與原來的特徵F相乘，就可以得到縮放後的新特徵。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/033.jpg)


* Spatial attention module

SAM 則是將注意力機制用於空間中，會沿著 channel 軸分別進行 Average pooling 及 Max pooling，期望能夠藉由注意力機制找出需要關注的特徵位於哪個位置。

其做法為分別對不同特徵圖上相同位置的像素值進行全局的 Maxpooling 與 Averagepooling 後，進行 concatenate，再通過一個 7x7 的卷積層以及 Sigmoid 得到權重系數 Ms。
最後再將這個權重系數與原來的特徵F相乘，就可以得到縮放後的新特徵。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/034.jpg)

* YOLO SAM

接著回過頭來看 YOLO 架構中使用的 SAM 模塊，將 Spatial attention module 更改為 Pointwise Attention，去除了 Maxpooling 與 Averagepooling 的操作，改成一個 7x7 的卷積層，通過 Sigmoid 後與原來的特徵進行逐點相乘。

<div align=center><img width="500" height="400" src="https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/035.jpg"/></div>


## 詳細論文閱讀筆記

若想更了解 YOLO 論文的細節，可閱讀以下文章。

[YOLO演進 — 1](https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-1-33220ebc1d09)

[YOLO演進 — 2](https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-2-85ee99d114a1)

[YOLO演進 — 3 — YOLOv4詳細介紹](https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-3-yolov4%E8%A9%B3%E7%B4%B0%E4%BB%8B%E7%B4%B9-5ab2490754ef)

[YOLO演進 — 4 — Scaled-YOLOv4](https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-4-scaled-yolov4-c8c361b4f33f)

[YOLO演進 — YOLOv7 論文閱讀](https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-yolov7-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-97b0e914bdbe)
