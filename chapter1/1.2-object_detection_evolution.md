# 演算法發展簡介

目前的物件偵測 Object Detection 演算法主要分為依 stage 分類或是依 anchor 分類兩部分。

前者是指在預測物件的類別及位置時，採用兩個階段還是一步到位的進行預測；後者則表示是否要使用 anchor 來提取物件框。

接著來看圖會更清晰，One-stage 為僅需要一個階段就可以預測物件類別和位置、Two-stage 則是要使用兩個階段才能得到最終結果，這兩者都是屬於 Anchor-based 方法，因為這兩種方式都是需要藉由 Anchor 來進行。

而 Anchor-free 是使用類似 segmentation 的方法來進行預測，目前 Anchor-free 模型主要分為基於 Keypoint 和基於 Anchor-Point。


![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/005.jpg)

## Two-stage vs One-stage

Two-Stage Detectors 是指要使用兩個階段才能得到最終預測結果，第一階段為 Region proposal，主要是用來找出可能是物件的位置，第二階段則是進行物件分類與優化物件邊框的位置及大小。

早期演算法以 Two-stage 為大宗，準確率比早期的 One-stage 演算法高，但在速度上相對較慢。代表演算法有 R-CNN、 Fast-RCNN和 Faster RCNN。


![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/006.jpg)


One-stage Detectors 的發展是為了改善 Two-stage 檢測速度問題，僅需要一個階段就可以同時預測物件類別和位置，因此速度比Two-stage 演算法快。

早期的One-stage 演算法準確率較差 (如 YOLOv1)，但在近年來準確率已超越了 Two-stage 演算法 (如 YOLOv2～YOLOV7)。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/007.jpg)

## Anchor-based vs Anchor-free

* Anchor-based

Anchor-based 是指需要用 Anchor 來預測物件類別和位置，首先會將原本的圖片分割成多個 grid cell (網格)，每個 cell 會採用一些不同大小的 Anchor 來幫助預測，這些 Anchor的大小及數量會事先定義好。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/008.jpg)

由於會產生非常多密集的 Anchor，有助於在小物體上的檢測。但同時容易造成正負樣本不均衡的問題，因為通常圖像的檢測目標沒有那麼多，而且也會增加運算複雜度。

此外，Anchor 的超參數是人工定義，能夠更容易且穩定的進行訓練。但是在設置上需要對任務資料有較強的了解，因此在不同的任務上都需要重新調整。

* Anchor-free

Anchor-free 如同其名不需要用 Anchor 來預測，而是使用類似 segmentation 的方法來預測物件類別和位置。也因為不用設置 Anchor 與 IOU 超參數，模型相對簡單，同時也避免了大量的 IOU 計算，能夠降低訓練時的記憶體使用率，但相對於 Anchor-based 演算法較不穩定。

Anchor-free 與 Anchor-based 演算法一樣都有正負樣本嚴重不均衡的問題，在2017 年Focal loss 提出後，有效地解決了這個問題。另外，Anchor-free 演算法有兩個物件的中心點可能落於同一個 grid cell 的問題，由於沒有 Anchor 能夠將其分開匹配，因此早期的演算法的準確率不及 Anchor-based，同樣在 2017 年 Feature Pyramid Network (FPN) 提出後解決了重合的問題。

目前 Anchor-free 模型主要分為基於 Keypoint 和基於 Anchor-Point，前者為利用關鍵點來進行偵測，源自於多人姿態估計論文的想法；後者為借鑑 FCN 的思想採用逐像素 (pixel-wise) 的方式來進行。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/009.jpg)


此外，在 CVPR2020 ATSS 論文中討論了 Anchor-free 與 Anchor-based 演算法之間的差異，作者認為主要是來自於正負樣本的選取方式不同。

下圖為 RetinaNet 和 FCOS 正負樣本選擇的對照圖。RetinaNet 選取正負樣本的方式是依據 IoU threshold 來劃分，作法是先將來自不同 pyramid levels 的 Anchor box 與 ground truth 計算 IoU 以劃分正負樣本 (IoU > threshold 為正樣本，反之為負樣本)。而 FCOS 則是先考慮 Anchor point 位於哪些 grid cell 中，再根據 pyramid level 的 scale range 決定正負樣本。

![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/010.jpg)


該論文還提出了 ATSS (Adaptive Training Sample Selection) 方法，根據每個 ground truth box，於不同 pyramid levels 上選擇離中心點最近的 k 個 Anchor box，計算 ground truth 與 Anchor box 之間的 IoU，再計算所有 IoU 的平均值與標準差。接著將這兩個數值相加作為 threshold 來挑選正樣本 (IoU > threshold 為正樣本，反之為負樣本)。藉由這樣的方式自動選擇合適的 Anchor 作為正樣本，能夠大幅提升模型的準確率並且不增加計算量。


![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/011.jpg)

## 模型總覽

最後放上整理好的物件偵測模型總覽。


![image](https://github.com/chingi071/AIoT_object_detection_tutorial/blob/main/chapter1/pictures/012.jpg)
